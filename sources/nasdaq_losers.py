import asyncio
import logging
from datetime import datetime
from .base_scraper import BaseFinvizScraper
from utils.api import post_stock_data

class NasdaqLosersScraper(BaseFinvizScraper):
    def __init__(self, max_retries=3, retry_delay=5):
        super().__init__(max_retries, retry_delay)
        self.url = "https://finviz.com/screener.ashx?v=111&s=ta_toplosers&f=exch_nasd&o=change"
    
    async def scrape(self):
        filtered_results = []
        
        p, browser, context = await self._setup_browser()
        
        try:
            page, success = await self._load_page_with_retry(context, self.url)
            if not success:
                logging.error("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                return filtered_results
            
            # Get all rows except header
            rows = await page.query_selector_all("table.styled-table-new tbody tr")
            logging.info(f"ğŸ“ ì´ {len(rows)}ê°œì˜ í–‰ ë°œê²¬")
            
            current_time = datetime.now()
            
            for row in rows:
                try:
                    # Extract data from each column
                    cells = await row.query_selector_all("td")
                    
                    # Get text content for each cell
                    symbol = await cells[1].text_content()
                    name = await cells[2].text_content()
                    change_percent = float((await cells[9].text_content()).strip('%'))
                    
                    # Format data according to the database schema
                    stock_data = {
                        "symbol": symbol,
                        "name": name,
                        "change": change_percent,  # ì´ë¯¸ ìŒìˆ˜ë¡œ ë“¤ì–´ì˜´
                        "type": "LOSER",
                        "index": "NASDAQ100",
                        "date": current_time.isoformat()
                    }
                    
                    if post_stock_data(stock_data):
                        filtered_results.append(stock_data)
                        logging.info(f"âœ… {symbol} ({name}) ìˆ˜ì§‘ ì™„ë£Œ")
                    
                    if len(filtered_results) >= 20:
                        logging.info("ğŸ¯ ëª©í‘œ ìˆ˜ëŸ‰(20ê°œ) ë‹¬ì„±")
                        break
                        
                except Exception as e:
                    logging.error(f"âŒ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
        except Exception as e:
            logging.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            await browser.close()
            await p.stop()
            logging.info("ğŸ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
            
        return filtered_results

async def scrape_and_filter_nasdaq_losers():
    scraper = NasdaqLosersScraper()
    return await scraper.scrape()

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    results = asyncio.run(scrape_and_filter_nasdaq_losers())
    print(f"ğŸ‰ ì„±ê³µì ìœ¼ë¡œ {len(results)}ê°œì˜ ë‚˜ìŠ¤ë‹¥ í•˜ë½ ì¢…ëª© ì²˜ë¦¬ ì™„ë£Œ") 